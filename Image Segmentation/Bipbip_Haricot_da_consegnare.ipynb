{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bipbip_Haricot_da_conseganre.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CdLB_y9M1CEC"},"source":["This model is our most succesful one for Bipbip Haricot images. We use the transefer learning and in particular the efficientnet architecture. We have to install it from the web and we use the B5 one, best compromise between speed and accuracy. We reached a score of 0.7154 in the test set."]},{"cell_type":"code","metadata":{"id":"K_OTsmhkYJy_"},"source":["from IPython.core.interactiveshell import InteractiveShell\r\n","InteractiveShell.ast_node_interactivity = \"all\" "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BsogHiON1xfV"},"source":["We import libraries, set the seed and the working directory."]},{"cell_type":"code","metadata":{"id":"S4BsdLiAYKL7"},"source":["import os \r\n","\r\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \r\n","import tensorflow as tf\r\n","import numpy as np\r\n","\r\n","# Set the seed for random operations. \r\n","# This let our experiments to be reproducible. \r\n","SEED = 1234\r\n","tf.random.set_seed(SEED) \r\n","np.random.seed(1234)\r\n","\r\n","cwd = os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsYR28PCYKQW"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WskLM3Lq_Zjs"},"source":["We have created a folder 'Challenge2' in our drive with the dataset and the starting kit folder"]},{"cell_type":"code","metadata":{"id":"nK5_OUMhYKSn"},"source":["!unzip /content/drive/My\\ Drive/Challenge2/Development_Dataset.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTEBe_HB2BFe"},"source":["We build the generator.\r\n","After some trials we have found that it was better to not use data augmentation, due to specific features in all the images."]},{"cell_type":"code","metadata":{"id":"Z3W51HeQYKUA"},"source":["# ImageDataGenerator\r\n","# ------------------\r\n","\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","\r\n","apply_data_augmentation = True\r\n","\r\n","# Create training ImageDataGenerator object\r\n","# We need two different generators for images and corresponding masks\r\n","if apply_data_augmentation:\r\n","    img_data_gen = ImageDataGenerator(rescale=1./255, \r\n","                                      horizontal_flip=False,\r\n","                                      vertical_flip=False,\r\n","                                      fill_mode='reflect')\r\n","    mask_data_gen = ImageDataGenerator(horizontal_flip=False,\r\n","                                       vertical_flip=False,\r\n","                                       fill_mode='reflect')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_mY2tq64IeR"},"source":["We fix the dimensions of the images, we tried to keep it as big as possible to not lose features"]},{"cell_type":"code","metadata":{"id":"T0Y-_8_lYKWC"},"source":["img_h = 1024 \r\n","img_w = 1024 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rirc6eEf4LGD"},"source":["Function of the starting kit used to read the mask images with a little modification in order to do the correct reshape of the images"]},{"cell_type":"code","metadata":{"id":"Mp3wJrprYKX8"},"source":["import os\r\n","import numpy as np\r\n","from PIL import Image\r\n","\r\n","\r\n","def read_rgb_mask(img_path):\r\n","    '''\r\n","    img_path: path to the mask file\r\n","    Returns the numpy array containing target values\r\n","    '''\r\n","\r\n","    mask_img = Image.open(img_path)\r\n","    mask_img = mask_img.resize([img_h,img_w])\r\n","    mask_arr = np.array(mask_img)\r\n","\r\n","    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\r\n","\r\n","    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\r\n","    new_mask_arr[np.where(np.all(mask_arr == [254, 124, 18], axis=-1))] = 0\r\n","    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\r\n","    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\r\n","    new_mask_arr=new_mask_arr.reshape((img_h,img_w,1))\r\n","\r\n","    return new_mask_arr\r\n","\r\n","\r\n","#if __name__ == \"__main__\":\r\n","\r\n","    # Read the example RGB mask and transform it into integer labels.\r\n","\r\n","    #mask = read_rgb_mask(\"/content/drive/My Drive/Challenge2/starting_kit/predictions/rgb_mask_example.png\")\r\n","\r\n","    #np.save(\"/content/drive/My Drive/Challenge2/starting_kit/predictions/arr_mask_example.npy\", mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZwR-pfL4bDD"},"source":["Function to create the custom dataset where we read the images from the selected dataset. We have decided to focus ourselves on one dataset at a time (here Bipbip Haricot)"]},{"cell_type":"code","metadata":{"id":"iRVC46S8YKaO"},"source":["from PIL import Image\r\n","\r\n","class CustomDataset(tf.keras.utils.Sequence):\r\n","\r\n","  \"\"\"\r\n","    CustomDataset inheriting from tf.keras.utils.Sequence.\r\n","\r\n","    3 main methods:\r\n","      - __init__: save dataset params like directory, filenames..\r\n","      - __len__: return the total number of samples in the dataset\r\n","      - __getitem__: return a sample from the dataset\r\n","\r\n","    Note: \r\n","      - the custom dataset return a single sample from the dataset. Then, we use \r\n","        a tf.data.Dataset object to group samples into batches.\r\n","      - in this case we have a different structure of the dataset in memory. \r\n","        We have all the images in the same folder and the training and validation splits\r\n","        are defined in text files.\r\n","\r\n","  \"\"\"\r\n","\r\n","  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \r\n","               preprocessing_function=None, out_shape=None):\r\n","    if which_subset == 'training':\r\n","      subset_file = os.path.join(dataset_dir, 'training.txt')\r\n","    elif which_subset == 'validation':\r\n","      subset_file = os.path.join(dataset_dir, 'validation.txt')\r\n","    \r\n","    with open(subset_file, 'r') as f:\r\n","      lines = f.readlines()\r\n","    \r\n","    subset_filenames = []\r\n","    for line in lines:\r\n","      subset_filenames.append(line.strip()) \r\n","\r\n","    self.which_subset = which_subset\r\n","    self.dataset_dir = dataset_dir\r\n","    self.subset_filenames = subset_filenames\r\n","    self.img_generator = img_generator\r\n","    self.mask_generator = mask_generator\r\n","    self.preprocessing_function = preprocessing_function\r\n","    self.out_shape = out_shape\r\n","\r\n","  def __len__(self):\r\n","    return len(self.subset_filenames)\r\n","\r\n","  def __getitem__(self, index):\r\n","    # Read Image\r\n","    curr_filename = self.subset_filenames[index]\r\n","    img = Image.open(os.path.join('/content/Development_Dataset/Training/Bipbip/Haricot/Images', curr_filename + '.jpg'))\r\n","    \r\n","    # Resize image and mask\r\n","    img = img.resize(self.out_shape)\r\n","    \r\n","    \r\n","    img_arr = np.array(img)\r\n","\r\n","    mask_arr = read_rgb_mask(os.path.join('/content/Development_Dataset/Training/Bipbip/Haricot/Masks', curr_filename + '.png'))\r\n","\r\n","    #if self.which_subset == 'training':\r\n","    if self.img_generator is not None and self.mask_generator is not None:\r\n","        # Perform data augmentation\r\n","        # We can get a random transformation from the ImageDataGenerator using get_random_transform\r\n","        # and we can apply it to the image using apply_transform\r\n","        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\r\n","        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\r\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\r\n","        # ImageDataGenerator use bilinear interpolation for augmenting the images.\r\n","        # Thus, when applied to the masks it will output 'interpolated classes', which\r\n","        # is an unwanted behaviour. As a trick, we can transform each class mask \r\n","        # separately and then we can cast to integer values (as in the binary segmentation notebook).\r\n","        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\r\n","        out_mask = np.zeros_like(mask_arr)\r\n","        for c in np.unique(mask_arr):\r\n","          if c > 0:\r\n","            curr_class_arr = np.float32(mask_arr == c)\r\n","            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\r\n","            # from [0, 1] to {0, 1}\r\n","            curr_class_arr = np.uint8(curr_class_arr)\r\n","            # recover original class\r\n","            curr_class_arr = curr_class_arr * c \r\n","            out_mask += curr_class_arr\r\n","    #else:\r\n","      #out_mask = mask_arr\r\n","    \r\n","    if self.preprocessing_function is not None:\r\n","        img_arr = self.preprocessing_function(img_arr)\r\n","\r\n","    return img_arr, np.float32(out_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PF7dmtVw4mN7"},"source":["Here we create the two txt files with the train and validation split"]},{"cell_type":"code","metadata":{"id":"_-6LToWmYKcj"},"source":["from os import listdir\r\n","from os.path import isfile, join\r\n","onlyfiles = [f for f in listdir(\"/content/Development_Dataset/Training/Bipbip/Haricot/Images\") if isfile(join(\"/content/Development_Dataset/Training/Bipbip/Haricot/Images\", f))]\r\n","for f in onlyfiles:\r\n","  f=f[:-4]\r\n","onlyfiles[:] = [f[:-4] for f in onlyfiles]\r\n","np.random.shuffle(onlyfiles)\r\n","\r\n","valid_split=0.2\r\n","\r\n","trainlength=int(len(onlyfiles)*(1-valid_split))\r\n","\r\n","import os \r\n","  \r\n","directory = \"Bipbip_Haricot\"\r\n","  \r\n","parent_dir = \"/content/drive/My Drive/\"\r\n","  \r\n","path = os.path.join(parent_dir, directory) \r\n","os.mkdir(path) \r\n","\r\n","\r\n","with open(\"/content/drive/My Drive/Bipbip_Haricot/training.txt\", \"w\") as file: \r\n","  file.write(\"\\n\".join(onlyfiles[0:(trainlength-1)]))\r\n","\r\n","with open(\"/content/drive/My Drive/Bipbip_Haricot/validation.txt\", \"w\") as file: \r\n","  file.write(\"\\n\".join(onlyfiles[trainlength:-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-241z51q4pm2"},"source":["Now we can use the function defiend before to build our custom dataset."]},{"cell_type":"code","metadata":{"id":"ksCHtNCRYKet"},"source":["dataset = CustomDataset('/content/drive/My Drive/Bipbip_Haricot', 'training', \r\n","                        img_generator=img_data_gen, mask_generator=mask_data_gen,\r\n","                        preprocessing_function=None, out_shape=[img_h, img_w])\r\n","dataset_valid = CustomDataset('/content/drive/My Drive/Bipbip_Haricot', 'validation', \r\n","                              img_generator=img_data_gen, mask_generator=mask_data_gen,\r\n","                              preprocessing_function=None, out_shape=[img_h, img_w])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q59lGFks4t1C"},"source":["Now we can use the defined generators. Set on repeat in order to reuse the same images. We selected a single image for batch becasue we had some problem of memory with larger batch size."]},{"cell_type":"code","metadata":{"id":"ZGpvqk61YKhI"},"source":["train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\r\n","                                               output_types=(tf.float32, tf.float32),\r\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n","\r\n","train_dataset = train_dataset.batch(1)\r\n","\r\n","train_dataset = train_dataset.repeat()\r\n","\r\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\r\n","                                               output_types=(tf.float32, tf.float32),\r\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n","valid_dataset = valid_dataset.batch(1)\r\n","\r\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZddvuhA49G8"},"source":["With the next three chunks we control that everything is done correctly and we give a look to the dataset we have built."]},{"cell_type":"code","metadata":{"id":"X3OqtiMXYKjH"},"source":["# Let's test data generator\r\n","# -------------------------\r\n","import time\r\n","from matplotlib import cm\r\n","import matplotlib.pyplot as plt\r\n","\r\n","%matplotlib inline\r\n","\r\n","# Assign a color to each class\r\n","evenly_spaced_interval = np.linspace(0, 1, 3)\r\n","colors = [cm.rainbow(x) for x in evenly_spaced_interval]\r\n","\r\n","iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cI5oyJTYKlL"},"source":["augmented_img, target = next(iterator)\r\n","counts=np.unique(target,return_counts=True)\r\n","counts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j55gCYsEYKnF"},"source":["fig, ax = plt.subplots(1, 2)\r\n","\r\n","augmented_img, target = next(iterator)\r\n","augmented_img = augmented_img[0]   # First element\r\n","augmented_img = augmented_img  # denormalize\r\n","\r\n","target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\r\n","\r\n","print(np.unique(target))\r\n","\r\n","target_img = np.zeros([target.shape[0], target.shape[1], 3])\r\n","\r\n","target_img[np.where(target == 0)] = [0, 0, 0]\r\n","for i in range(1, 3):\r\n","  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255\r\n","\r\n","ax[0].imshow(np.uint8(augmented_img))\r\n","ax[1].imshow(np.uint8(target_img))\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRWCS1H-5bbc"},"source":["We install efficientnet in order to use it for transfer learning."]},{"cell_type":"code","metadata":{"id":"vJ-EselFZB0G"},"source":["!pip install efficientnet\r\n","import efficientnet.tfkeras as efn "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6v-lqzU5i2t"},"source":["Now we create the model. We use Imagenet weights but completely fine-tuned to recover the features useful on this task, and a decoder with transpose convolutions and with dropout to balance the great complexity of the net.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"-937-4upZB2Y"},"source":["from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization, Dropout\r\n","from tensorflow.keras.layers import UpSampling2D, Input, Concatenate, Conv2DTranspose\r\n","from tensorflow.keras.models import Model\r\n","\r\n","def model():\r\n","    inputs = Input(shape=(img_h,img_w, 3), name=\"input_image\")\r\n","    \r\n","    encoder = efn.EfficientNetB5(input_tensor=inputs, include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3))\r\n","    skip_connection_names = [\"input_image\", \"block2a_expand_activation\", \"block3a_expand_activation\", \"block4a_expand_activation\", \"block6a_expand_activation\"]\r\n","    encoder_output = encoder.get_layer(\"top_activation\").output\r\n","    \r\n","    f = [16, 32, 64, 128, 128] \r\n","    x = encoder_output\r\n","    for i in range(1, len(skip_connection_names)+1, 1):\r\n","        x_skip = encoder.get_layer(skip_connection_names[-i]).output\r\n","        x = Conv2DTranspose(f[-i], (2, 2), strides=(2, 2), padding='same')(x)\r\n","        x = Concatenate()([x, x_skip])\r\n","        x = BatchNormalization()(x)\r\n","        \r\n","        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\r\n","        x = BatchNormalization()(x)\r\n","        x = Activation(\"relu\")(x)\r\n","        x = Dropout(0.1)(x)\r\n","        \r\n","        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\r\n","        x = BatchNormalization()(x)\r\n","        x = Activation(\"relu\")(x)\r\n","        x = Dropout(0.1)(x)\r\n","        \r\n","    x = Conv2D(3, (1, 1), padding=\"same\")(x)\r\n","    x = Activation(\"sigmoid\")(x)\r\n","    \r\n","    model = Model(inputs, x)\r\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hqm89E_dZB4q"},"source":["model = model()\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"igEy8b148cT6"},"source":["We define the meanIoU and compile the model"]},{"cell_type":"code","metadata":{"id":"EqaTUPvZZB7p"},"source":["# Optimization params\r\n","# -------------------\r\n","\r\n","# Loss\r\n","# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\r\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \r\n","\r\n","# learning rate\r\n","lr = 1e-3   # After some trials, we have found this lr to be good enough to train quickly \r\n","            # without oscillating too much around the final solution\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)  \r\n","# -------------------\r\n","\r\n","# Here we define the intersection over union for each class in the batch.\r\n","# Then we compute the final IoU as the mean over classes\r\n","def meanIoU(y_true, y_pred):\r\n","    # get predicted class from softmax\r\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n","\r\n","    per_class_iou = []\r\n","\r\n","    for i in range(1,3): # exclude the background class 0\r\n","      # Get prediction and target related to only a single class (i)\r\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n","      intersection = tf.reduce_sum(class_true * class_pred)\r\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n","    \r\n","      iou = (intersection + 1e-7) / (union + 1e-7)\r\n","      per_class_iou.append(iou)\r\n","\r\n","    return tf.reduce_mean(per_class_iou)\r\n","\r\n","# Validation metrics\r\n","# ------------------\r\n","metrics = ['accuracy', meanIoU]\r\n","# ------------------\r\n","\r\n","# Compile Model\r\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ixDI4YhE8iWY"},"source":["We add some callbacks: early stopping to stop overfit and the learning rate adapter to refine search for minima during training. Then we train the model. (Normally it stops much before 80 epochs)"]},{"cell_type":"code","metadata":{"id":"UFeGbZbfZB_A"},"source":["import os\r\n","from datetime import datetime\r\n","\r\n","callbacks = []\r\n","\r\n","\r\n","# earlystopping callback\r\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_meanIoU', mode='max', patience=10,restore_best_weights=True)\r\n","callbacks.append(es_callback)\r\n","# learning rate adapter callback\r\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_meanIoU', mode='max', factor=0.2, patience=4, verbose=1, min_delta=0.0001, cooldown=0)\r\n","callbacks.append(LR_adapter_callback)\r\n","    \r\n","model.fit(x=train_dataset,\r\n","          epochs=80,  \r\n","          steps_per_epoch=len(dataset),\r\n","          validation_data=valid_dataset,\r\n","          validation_steps=len(dataset_valid), \r\n","          callbacks=callbacks,\r\n","          verbose = 1) #We like to monitor the meanIoU during training live ()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BZcf7nlGRYi"},"source":["In order to converge more precisely we change the optimizer and we contiune the training using Adadelta instead of Adam."]},{"cell_type":"code","metadata":{"id":"Uq4rOkZkGUs5"},"source":["# Optimization params\r\n","# -------------------\r\n","\r\n","# Loss\r\n","# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\r\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \r\n","\r\n","# learning rate\r\n","lr = 1e-3\r\n","optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\r\n","\r\n","# Validation metrics\r\n","# ------------------\r\n","metrics = ['accuracy', meanIoU]\r\n","# ------------------\r\n","\r\n","# Compile Model\r\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyFyljnjGaxf"},"source":["We train the model with the same callbacks as before."]},{"cell_type":"code","metadata":{"id":"gXBkLhloGdHp"},"source":["import os\r\n","from datetime import datetime\r\n","\r\n","callbacks = []\r\n","\r\n","\r\n","# earlystopping callback\r\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_meanIoU', mode='max', patience=6,restore_best_weights=True)\r\n","callbacks.append(es_callback)\r\n","# learning rate adapter callback\r\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_meanIoU', mode='max', factor=0.2, patience=4, verbose=1, min_delta=0.0001, cooldown=0)\r\n","callbacks.append(LR_adapter_callback)\r\n","    \r\n","model.fit(x=train_dataset,\r\n","          epochs=80,  \r\n","          steps_per_epoch=len(dataset),\r\n","          validation_data=valid_dataset,\r\n","          validation_steps=len(dataset_valid), \r\n","          callbacks=callbacks,\r\n","          verbose = 1) #We like to monitor the accuracy during training live ()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ASCzC-U8pEV"},"source":["To save the model"]},{"cell_type":"code","metadata":{"id":"5EBkrMj3ZCBh"},"source":["#model.save_weights(os.path.join('/content/drive/My Drive/ModelloBipbipHaricot'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cx5JCSoC8rNs"},"source":["To load the model"]},{"cell_type":"code","metadata":{"id":"j-n6MkCoZN0A"},"source":["#model.load_weights('/content/drive/My Drive/ModelloBipbipHaricot')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oj6QeQaK8tfW"},"source":["Now we make predictions on the test set and we create a json file with the results."]},{"cell_type":"code","metadata":{"id":"CUDbOmigZN2c"},"source":["import os\r\n","import json\r\n","from os import listdir\r\n","import numpy as np\r\n","from PIL import Image\r\n","\r\n","\r\n","def rle_encode(img):\r\n","    '''\r\n","    img: numpy array, 1 - foreground, 0 - background\r\n","    Returns run length as string formatted\r\n","    '''\r\n","    pixels = img.flatten()\r\n","    pixels = np.concatenate([[0], pixels, [0]])\r\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\r\n","    runs[1::2] -= runs[::2]\r\n","    return ' '.join(str(x) for x in runs)\r\n","\r\n","submission_dict = {}\r\n","\r\n","teams = ['Roseau','Bipbip','Pead','Weedelec']\r\n","plants = ['Haricot','Mais']\r\n","\r\n","for team in teams:\r\n","  for plant in plants:\r\n","    test_dir = os.path.join('/content/Development_Dataset/Test_Dev', team, plant)\r\n","    img_folder_dir = os.path.join(test_dir, 'Images')\r\n","    imgs = listdir(img_folder_dir)\r\n","    imgs = [f[:-4] for f in imgs]\r\n","    for img_name in imgs:\r\n","      if team == 'Roseau':\r\n","        img = Image.open(os.path.join(test_dir, 'Images', img_name + '.png')).convert('RGB')\r\n","      else:\r\n","        img = Image.open(os.path.join(test_dir, 'Images', img_name + '.jpg')).convert('RGB')\r\n","      original_size=img.size\r\n","      img = img.resize((img_h, img_w))\r\n","      img_array = np.array(img)\r\n","\r\n","      out_sigmoid = model.predict(tf.expand_dims(img_array, axis=0))\r\n","      mask_arr = tf.argmax(out_sigmoid, -1) \r\n","\r\n","      mask_arr=tf.reshape(mask_arr,[1,img_h,img_w,1])\r\n","      mask_arr=tf.compat.v1.image.resize_nearest_neighbor(mask_arr,original_size[::-1])\r\n","      mask_array=np.array(mask_arr)\r\n","      mask_array=np.reshape(mask_array,original_size[::-1])\r\n","      \r\n","      submission_dict[img_name] = {}\r\n","      submission_dict[img_name]['shape'] = mask_array.shape\r\n","      submission_dict[img_name]['team'] = team  \r\n","      submission_dict[img_name]['crop'] = plant \r\n","      submission_dict[img_name]['segmentation'] = {}\r\n","\r\n","      # RLE encoding\r\n","      # crop\r\n","      rle_encoded_crop = rle_encode(mask_array == 1)\r\n","      # weed\r\n","      rle_encoded_weed = rle_encode(mask_array == 2)\r\n","\r\n","      submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\r\n","      submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\r\n","\r\n","# Finally, save the results into the submission.json file\r\n","with open('/content/drive/My Drive/submission.json', 'w') as f:\r\n","    json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXL0Vgc188vC"},"source":["With this code we inspect also visually the goodnes of our predictions."]},{"cell_type":"code","metadata":{"id":"WIVUFxX7ZVzU"},"source":["# To visualize the output \r\n","import os\r\n","import json\r\n","from os import listdir\r\n","import numpy as np\r\n","from PIL import Image\r\n","\r\n","teams = 'Bipbip'\r\n","plants = 'Haricot'\r\n","\r\n","\r\n","test_dir = os.path.join('/content/Development_Dataset/Test_Dev', teams, plants)\r\n","img_folder_dir = os.path.join(test_dir, 'Images')\r\n","imgs = listdir(img_folder_dir)\r\n","imgs = [f[:-4] for f in imgs]\r\n","img_name = imgs[0] #here changing this index we look at the predictions of different images in the testset\r\n","if teams == 'Roseau':\r\n","  img = Image.open(os.path.join(test_dir, 'Images', img_name + '.png')).convert('RGB')\r\n","else:\r\n","  img = Image.open(os.path.join(test_dir, 'Images', img_name + '.jpg')).convert('RGB')\r\n","original_size=img.size\r\n","img1 = img.resize((img_h, img_w))\r\n","img_array = np.array(img1)\r\n","out_sigmoid = model.predict(tf.expand_dims(img_array, axis=0))\r\n","mask_arr = tf.argmax(out_sigmoid, -1)\r\n","mask_arr=tf.reshape(mask_arr,[1,img_h,img_w,1])\r\n","mask_arr=tf.compat.v1.image.resize_nearest_neighbor(mask_arr, original_size[::-1])\r\n","mask_array=np.array(mask_arr)\r\n","mask_array=np.reshape(mask_array,original_size[::-1])\r\n","target_img = np.zeros([mask_array.shape[0], mask_array.shape[1], 3])\r\n","\r\n","target_img[np.where(mask_array == 0)] = [0, 0, 0]\r\n","for i in range(1, 3):\r\n","  target_img[np.where(mask_array == i)] = np.array(colors[i-1])[:3] * 255\r\n","fig, ax = plt.subplots(1, 2,figsize=(20, 10))\r\n","ax[0].imshow(np.uint8(np.array(img)))\r\n","ax[1].imshow(np.uint8(target_img))\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]}]}